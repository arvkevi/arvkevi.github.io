{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Motivation for this post originates from my interest in working with [openSNP](https://opensnp.org/) data. openSNP is an open-source platform where users upload their genotype data from direct-to-consumer genetics companies like [23andMe](https://www.23andme.com/).  Raw result files are often hundreds of thousandas of records long, each record representing a location in the human genome.  There are over 4,000 raw result files in the openSNP data however, my laptop runs a VirtualBox with only 5GB RAM, which meant I needed to find some clever solutions beyond the standard [pandas]() library in order to work with this much data.  This post describes one of several applications that can benefit from out-of-core computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openSNP data dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this will take some time to download, depending on your internet speed\n",
    "! wget https://opensnp.org/data/zip/opensnp_datadump.current.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! unzip opensnp_datadump.current.zip\n",
    "! du -h "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's over 40 GB of genotype data stored as text files!  For the purposes of this demo, the analysis will be limited to only the **23&Me** files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Isolate the 23&me files\n",
    "! mkdir /media/sf_ubuntuVbox/gt23\n",
    "! mv *.23andme.txt* /media/sf_ubuntuVbox/gt23/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1915\r\n"
     ]
    }
   ],
   "source": [
    "! find /media/sf_ubuntuVbox/opensnp_datadump.current/gt23/ -type f | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a look at an example **23&Me** result file, the header is commented out with `#` then there are 600,000+ rows of genotype data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# This data file generated by 23andMe at: Mon Apr 28 21:55:49 2014\r",
      "\r\n",
      "#\r",
      "\r\n",
      "# Below is a text version of your data.  Fields are TAB-separated\r",
      "\r\n",
      "# Each line corresponds to a single SNP.  For each SNP, we provide its identifier \r",
      "\r\n",
      "# (an rsid or an internal id), its location on the reference human genome, and the \r",
      "\r\n",
      "# genotype call oriented with respect to the plus strand on the human reference sequence.\r",
      "\r\n",
      "# We are using reference human assembly build 37 (also known as Annotation Release 104).\r",
      "\r\n",
      "# Note that it is possible that data downloaded at different times may be different due to ongoing \r",
      "\r\n",
      "# improvements in our ability to call genotypes. More information about these changes can be found at:\r",
      "\r\n",
      "# https://www.23andme.com/you/download/revisions/\r",
      "\r\n",
      "# \r",
      "\r\n",
      "# More information on reference human assembly build 37 (aka Annotation Release 104):\r",
      "\r\n",
      "# http://www.ncbi.nlm.nih.gov/mapview/map_search.cgi?taxid=9606\r",
      "\r\n",
      "#\r",
      "\r\n",
      "# rsid\tchromosome\tposition\tgenotype\r",
      "\r\n",
      "rs12564807\t1\t734462\tAA\r",
      "\r\n",
      "rs3131972\t1\t752721\tGG\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "testfile = '/media/sf_ubuntuVbox/opensnp_datadump.current/gt23/user2001_file1185_yearofbirth_unknown_sex_unknown.23andme.txt'\n",
    "! head -20 $testfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602366 /media/sf_ubuntuVbox/opensnp_datadump.current/gt23/user2001_file1185_yearofbirth_unknown_sex_unknown.23andme.txt\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l $testfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, like most real-world data, the openSNP data dump was a little messy.  Some of the files were in binary format, had incorrect filename extensions, and malformed headers.  I removed files that were small or large, relative to the averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean 23&me files -- may need some other processing\n",
    "! find . -name \"*.txt\" -size -15M -delete\n",
    "! find . -name \"*.txt\" -size +25M -delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## openSNP test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first exercise, let's use a manageable subset of the full openSNP data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select 10 for a test set\n",
    "! mkdir gt23test\n",
    "! cp gt23/user200* gt23test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "dir23gt = '/media/sf_ubuntuVbox/opensnp_datadump.current/gt23test/'\n",
    "allFiles = glob.glob(dir23gt + \"/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166M\t/media/sf_ubuntuVbox/opensnp_datadump.current/gt23test/\r\n"
     ]
    }
   ],
   "source": [
    "# How big are the files in the gt23test directory?\n",
    "! du -h $dir23gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, start with a pure [pandas](http://pandas.pydata.org/) `read_csv` solution, something that should be familiar to Python data scientists.  Let's try to create a large `DataFrame` in memory.  `join` can accomplish this task, even though it's an expensive operation the test data is small enough that we can successfully execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.3 s, sys: 508 ms, total: 52.8 s\n",
      "Wall time: 53.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gtdf = pd.DataFrame(index=['rsid']) #empty DataFrame\n",
    "\n",
    "for file_ in allFiles:\n",
    "    #extract the userid\n",
    "    match = re.search('user(\\d+)', file_)\n",
    "    userid = match.group(1)\n",
    "    \n",
    "    df = pd.read_csv(file_, \n",
    "                     comment='#', \n",
    "                     sep='\\t', \n",
    "                     header=None,\n",
    "                     usecols=[0,3],\n",
    "                     error_bad_lines=False,\n",
    "                     names=['rsid', userid],\n",
    "                     dtype={'rsid':object,\n",
    "                            'genotype':object},\n",
    "                     index_col=['rsid']\n",
    "                    )\n",
    "    gtdf = gtdf.join(df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>i1000001</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i1000003</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i1000007</th>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i1000008</th>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i1000009</th>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         2000 2001 2002 2003 2004 2005 2006 2008 2009  200\n",
       "i1000001    A    A  NaN    A    A    A    A    A    A  NaN\n",
       "i1000003    A    A  NaN    A    A    A    A    A    A  NaN\n",
       "i1000007    C    C  NaN    C    C    C    C    C    C  NaN\n",
       "i1000008    G    G  NaN    G    G    G    G    G    G  NaN\n",
       "i1000009    G    G    G    G    G    G    G    G    G    G"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On the test set of 10 files, the entire `DataFrame` fits into memory.\n",
    "However, using this pure pandas method on the full set of 1,915 files would eventually \n",
    "crash the computer because it would run out of pyhsical memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask -- parallel out-of-core DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Enter [dask](http://dask.pydata.org/en/latest/index.html), a parallel Python library that implements out-of-core DataFrames.  It's API is similar to pandas, with a few additional methods and arguments.\n",
    "This code will read the same files in parallel and create a \"lazy\" `DataFrame` that isn't computed until explicity executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for the blocksize parameter in the next cell\n",
    "import psutil\n",
    "blocksize = psutil.virtual_memory().total / psutil.cpu_count() / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 8 ms, total: 24 ms\n",
      "Wall time: 35.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time # <-- won't actually read the csv's yet...\n",
    "ddf = dd.read_csv('/media/sf_ubuntuVbox/opensnp_datadump.current/gt23test/*.txt',\n",
    "                  comment='#', \n",
    "                  sep='\\t', \n",
    "                  header=None,\n",
    "                  usecols=[0,3],\n",
    "                  error_bad_lines=False,\n",
    "                  names=['rsid', 'genotype'],\n",
    "                  dtype={'rsid':object,\n",
    "                         'genotype':object},\n",
    "                  blocksize=blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dd.DataFrame<from-de..., npartitions=10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 partition per file\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.4 s, sys: 1.6 s, total: 29 s\n",
      "Wall time: 34.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#  this operation is expensive\n",
    "ddf = ddf.set_index('rsid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dask `compute()` method provides familiar results.  Since we didn't `join` the dask DataFrame, let's investigate one SNP.\n",
    "Remember, in contrast to `gtdf`, `ddf` isn't in memory so it will take longer to get this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.3 s, sys: 1.32 s, total: 19.6 s\n",
      "Wall time: 17.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CC    8\n",
       "CT    2\n",
       "Name: genotype, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dask\n",
    "%%time\n",
    "ddf.loc['rs1333525']['genotype'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 372 ms, sys: 0 ns, total: 372 ms\n",
      "Wall time: 383 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CC    8\n",
       "CT    2\n",
       "Name: rs1333525, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pure pandas\n",
    "%%time\n",
    "gtdf.loc['rs1333525'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first blush, the computation times are deceiving.  As expected, the dask method took longer because of the lazy computation, it still had to read all the files and then perform the operation.\n",
    "If you consider the 34 seconds to set the index and the 17.5 seconds to compute the `value_counts()`, the parallel dask method was actually equally as fast as the 53.9 seconds \n",
    "that it took pure pandas to create the `gtdf` `DataFrame`.  The benefit is that the parallel dask method didn't store the `DataFrame` object in memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Increase query peformance with Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It would be nice to speed up the dask queries so we can work with the `DataFrame` for downstream analysis.\n",
    "The solution is to store the data on disk in an efficient format (binary).  A popular choice for this is traditionally [HDF5](https://support.hdfgroup.org/HDF5/doc/H5.intro.html), but I chose to use [parquet](https://parquet.apache.org/) becuase \n",
    "HDF5 can be tricky to work with.  Dask uses the [fastparquet](http://fastparquet.readthedocs.io/en/latest/index.html) implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 3.74 s, total: 1min 11s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dd.to_parquet('/media/sf_ubuntuVbox/opensnp_datadump.current/gt23test_pq/', ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, what we've done to this point is convert csv files to parquet files.  How much performance is really gained from this?\n",
    "Revisiting the dask DataFrame from csv's, `ddf`, a comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using dask with fastparquet, nothing computed here...\n",
    "pqdf = dd.read_parquet('/media/sf_ubuntuVbox/opensnp_datadump.current/gt23test_pq/', index='rsid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.94 s, sys: 108 ms, total: 4.05 s\n",
      "Wall time: 4.37 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CC    8\n",
       "CT    2\n",
       "Name: genotype, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pqdf.loc['rs1333525']['genotype'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.6 s, sys: 1.15 s, total: 19.8 s\n",
      "Wall time: 18.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CC    8\n",
       "CT    2\n",
       "Name: genotype, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddf.loc['rs1333525']['genotype'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Parquet offers a noticeable performance increase, even with only 10 files.  Scaling this up to 1,915 files, the csv version, `ddf`, took 5+ hours to execute `value_counts()` for one SNP.\n",
    "`to_parquet` on the 1,915 file `DataFrame` took a couple hours.  Once you see the query performance improvements, it's well worth the up-front cost of converting from csv to parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1898\r\n"
     ]
    }
   ],
   "source": [
    "# Used commands above to produce parquet files\n",
    "! find /media/sf_ubuntuVbox/opensnp_datadump.current/gt23_pq/ -type f | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allpqdf = dd.read_parquet('/media/sf_ubuntuVbox/opensnp_datadump.current/gt23_pq/', index='rsid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.07 s, sys: 524 ms, total: 9.59 s\n",
      "Wall time: 10.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CC    1625\n",
       "CT     277\n",
       "TT      13\n",
       "Name: genotype, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "allpqdf.loc['rs1333525']['genotype'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results highlight the superior performance of parquet over csv.  Additionally, dask proved it's value as an easy-to-use tool when physical memory is a constraint.\n",
    "By now you've probably noticed I wasn't able to assign the user identifier to each column.  `dd.read_csv()` assumes the column names in each file are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient \"big-data\" downstream analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to show how easy dask and parquet make it to quickly compare variant frequencies in the opensnp data to [exAC](http://exac.broadinstitute.org/) data.\n",
    "Here we'll define a function to extract variant frequencies from exAC, then compare to openSNP frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ExacFrequency(rsid):\n",
    "    try:\n",
    "        exac = {}\n",
    "\n",
    "        request = urlopen('http://exac.hms.harvard.edu/rest/dbsnp/{}'.format(rsid))\n",
    "        results = request.read().decode('utf-8')\n",
    "        exac_variants = json.loads(results)['variants_in_region']\n",
    "\n",
    "        for alt_json in exac_variants:\n",
    "            exac_allele = alt_json['ref']+alt_json['alt']\n",
    "            exac_freq = alt_json['allele_freq']\n",
    "            exac[exac_allele] = exac_freq\n",
    "    except:\n",
    "        return {'n/a':0}\n",
    "    return exac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brca2snps = ['rs766173', 'rs144848', 'rs11571746', 'rs11571747',\n",
    "             'rs4987047', 'rs11571833', 'rs1801426']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['rsid', 'gt', 'source', 'varFreq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for snp in brca2snps:\n",
    "    Osnp_alleles = allpqdf.loc[snp]['genotype'].value_counts().compute() #dask\n",
    "    \n",
    "    Osnp_alleles = Osnp_alleles.to_dict()\n",
    "    opensnp_N = sum(Osnp_alleles.values())\n",
    "    \n",
    "    Osnp_freqs = {alt: AC/float(opensnp_N) for alt,AC in Osnp_alleles.items()}\n",
    "    exac_freqs= ExacFrequency(snp)\n",
    "    \n",
    "    for gt,freq in exac_freqs.items():\n",
    "        df = df.append({'rsid':snp, 'gt': gt, 'source':'exAC', 'varFreq':freq }, ignore_index=True )\n",
    "    \n",
    "    for gt,freq in Osnp_freqs.items():\n",
    "        if gt in exac_freqs:\n",
    "            gt = gt\n",
    "        elif gt[::-1] in exac_freqs:\n",
    "            gt = gt[::-1]\n",
    "        else:\n",
    "            continue\n",
    "        df = df.append({'rsid':snp, 'gt': gt, 'source':'Osnp', 'varFreq':freq }, ignore_index=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](/assets/images/snps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my next post, I'll focus on working with some of the phenotype groups identified in the opensnp data.  Thanks for reading, comments and code improvements welcome!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:matrixops]",
   "language": "python",
   "name": "conda-env-matrixops-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
